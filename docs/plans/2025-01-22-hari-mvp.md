# HARI MVP Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build an agentic RAG system with document ingestion, hybrid search, and basic query capabilities.

**Architecture:** FastAPI backend with PostgreSQL+pgvector, React+TypeScript frontend. Document pipeline: fetch → clean → extract → synthesize → embed → store. Hybrid search combines TSVector keyword + pgvector semantic with RRF fusion. Google OAuth + API keys for auth.

**Tech Stack:** Python 3.11+, FastAPI, SQLAlchemy (async), Alembic, pgvector, Sumy, Trafilatura, Claude/GPT-4, OpenAI embeddings, React, TypeScript, Vite, shadcn/ui, Tailwind CSS.

---

## Phase 1: Backend Project Setup

### Task 1.1: Initialize Python Project

**Files:**
- Create: `backend/pyproject.toml`
- Create: `backend/.env.example`
- Create: `backend/.gitignore`

**Step 1: Create backend directory structure**

```bash
mkdir -p backend/app/{api,core,models,schemas,services}
mkdir -p backend/tests
```

**Step 2: Create pyproject.toml**

```toml
[project]
name = "hari"
version = "0.1.0"
description = "Human-Augmented Resource Intelligence"
requires-python = ">=3.11"
dependencies = [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "sqlalchemy[asyncio]>=2.0.25",
    "asyncpg>=0.29.0",
    "alembic>=1.13.1",
    "pgvector>=0.2.4",
    "pydantic>=2.5.3",
    "pydantic-settings>=2.1.0",
    "python-multipart>=0.0.6",
    "httpx>=0.26.0",
    "anthropic>=0.18.0",
    "openai>=1.10.0",
    "trafilatura>=1.6.0",
    "sumy>=0.11.0",
    "nltk>=3.8.1",
    "pypdf2>=3.0.1",
    "langdetect>=1.0.9",
    "authlib>=1.3.0",
    "itsdangerous>=2.1.2",
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.4",
    "pytest-asyncio>=0.23.3",
    "pytest-cov>=4.1.0",
    "httpx>=0.26.0",
    "ruff>=0.1.14",
    "mypy>=1.8.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]

[tool.ruff]
line-length = 100
target-version = "py311"
```

**Step 3: Create .env.example**

```bash
# Database
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/hari

# LLM Providers
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...

# Google OAuth
GOOGLE_CLIENT_ID=...
GOOGLE_CLIENT_SECRET=...

# Security
SECRET_KEY=your-secret-key-here
ADMIN_API_KEY=your-admin-api-key

# Optional
JINA_API_KEY=jina_...
TAVILY_API_KEY=tvly-...
```

**Step 4: Create .gitignore**

```
__pycache__/
*.py[cod]
.env
.venv/
venv/
*.egg-info/
dist/
.pytest_cache/
.coverage
htmlcov/
.mypy_cache/
.ruff_cache/
```

**Step 5: Install dependencies**

Run: `cd backend && pip install -e ".[dev]"`

**Step 6: Commit**

```bash
git add backend/
git commit -m "feat: initialize backend project structure"
```

---

### Task 1.2: Core Configuration

**Files:**
- Create: `backend/app/core/config.py`
- Create: `backend/app/core/__init__.py`
- Test: `backend/tests/test_config.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_config.py
import pytest
from app.core.config import settings


def test_settings_loads_defaults():
    assert settings.app_name == "HARI"
    assert settings.environment in ["development", "staging", "production"]


def test_settings_database_url_required():
    assert settings.database_url is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_config.py -v`
Expected: FAIL with "ModuleNotFoundError"

**Step 3: Create __init__.py**

```python
# backend/app/core/__init__.py
```

**Step 4: Write minimal implementation**

```python
# backend/app/core/config.py
from pydantic_settings import BaseSettings
from functools import lru_cache


class Settings(BaseSettings):
    app_name: str = "HARI"
    environment: str = "development"
    debug: bool = False

    # Database
    database_url: str = "postgresql+asyncpg://localhost:5432/hari"

    # Security
    secret_key: str = "dev-secret-key-change-in-production"
    admin_api_key: str = "dev-admin-key"

    # LLM Providers
    anthropic_api_key: str | None = None
    openai_api_key: str | None = None

    # Google OAuth
    google_client_id: str | None = None
    google_client_secret: str | None = None

    # Optional services
    jina_api_key: str | None = None
    tavily_api_key: str | None = None

    model_config = {"env_file": ".env", "extra": "ignore"}


@lru_cache
def get_settings() -> Settings:
    return Settings()


settings = get_settings()
```

**Step 5: Run test to verify it passes**

Run: `cd backend && pytest tests/test_config.py -v`
Expected: PASS

**Step 6: Commit**

```bash
git add backend/app/core/ backend/tests/test_config.py
git commit -m "feat: add core configuration with pydantic-settings"
```

---

### Task 1.3: Database Connection

**Files:**
- Create: `backend/app/core/database.py`
- Test: `backend/tests/test_database.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_database.py
import pytest
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_session, engine


@pytest.mark.asyncio
async def test_engine_created():
    assert engine is not None


@pytest.mark.asyncio
async def test_get_session_yields_async_session():
    async for session in get_session():
        assert isinstance(session, AsyncSession)
        break
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_database.py -v`
Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write minimal implementation**

```python
# backend/app/core/database.py
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from app.core.config import settings

engine = create_async_engine(
    settings.database_url,
    echo=settings.debug,
    pool_pre_ping=True,
)

async_session_factory = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
)


async def get_session():
    async with async_session_factory() as session:
        yield session
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_database.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/core/database.py backend/tests/test_database.py
git commit -m "feat: add async database connection"
```

---

### Task 1.4: Base SQLAlchemy Model

**Files:**
- Create: `backend/app/models/__init__.py`
- Create: `backend/app/models/base.py`
- Test: `backend/tests/test_models_base.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_models_base.py
import pytest
from uuid import UUID
from datetime import datetime
from app.models.base import Base, TimestampMixin


def test_base_has_metadata():
    assert Base.metadata is not None


def test_timestamp_mixin_has_fields():
    assert hasattr(TimestampMixin, "created_at")
    assert hasattr(TimestampMixin, "updated_at")
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_models_base.py -v`
Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write minimal implementation**

```python
# backend/app/models/base.py
from datetime import datetime
from sqlalchemy import DateTime, func
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column


class Base(DeclarativeBase):
    pass


class TimestampMixin:
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        nullable=False,
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        onupdate=func.now(),
        nullable=False,
    )
```

```python
# backend/app/models/__init__.py
from app.models.base import Base, TimestampMixin

__all__ = ["Base", "TimestampMixin"]
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_models_base.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/models/
git commit -m "feat: add base SQLAlchemy model with timestamp mixin"
```

---

## Phase 1 Complete

Tasks 1.1-1.4 establish the backend foundation. Continue to Phase 2 for Document and User models.

---

## Phase 2: Database Models

### Task 2.1: Document Model

**Files:**
- Create: `backend/app/models/document.py`
- Modify: `backend/app/models/__init__.py`
- Test: `backend/tests/test_models_document.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_models_document.py
import pytest
from uuid import UUID
from app.models.document import Document, SourceType, ProcessingStatus


def test_document_has_required_fields():
    assert hasattr(Document, "id")
    assert hasattr(Document, "url")
    assert hasattr(Document, "title")
    assert hasattr(Document, "content")
    assert hasattr(Document, "summary")
    assert hasattr(Document, "embedding")


def test_source_type_enum():
    assert SourceType.URL.value == "url"
    assert SourceType.PDF.value == "pdf"
    assert SourceType.DRIVE.value == "drive"


def test_processing_status_enum():
    assert ProcessingStatus.PENDING.value == "pending"
    assert ProcessingStatus.COMPLETED.value == "completed"
    assert ProcessingStatus.FAILED.value == "failed"
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_models_document.py -v`
Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write minimal implementation**

```python
# backend/app/models/document.py
import enum
from uuid import UUID, uuid4
from sqlalchemy import String, Text, Enum, Float, Integer, JSON
from sqlalchemy.orm import Mapped, mapped_column
from pgvector.sqlalchemy import Vector
from app.models.base import Base, TimestampMixin


class SourceType(str, enum.Enum):
    URL = "url"
    PDF = "pdf"
    DRIVE = "drive"


class ProcessingStatus(str, enum.Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


class Document(Base, TimestampMixin):
    __tablename__ = "documents"

    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)

    # Source info
    url: Mapped[str | None] = mapped_column(String(2048))
    source_type: Mapped[SourceType] = mapped_column(Enum(SourceType), default=SourceType.URL)

    # Content
    title: Mapped[str | None] = mapped_column(String(500))
    content: Mapped[str | None] = mapped_column(Text)
    content_hash: Mapped[str | None] = mapped_column(String(64), unique=True)

    # Processed output
    summary: Mapped[str | None] = mapped_column(Text)
    quick_summary: Mapped[str | None] = mapped_column(String(500))
    keywords: Mapped[list | None] = mapped_column(JSON)
    industries: Mapped[list | None] = mapped_column(JSON)
    language: Mapped[str | None] = mapped_column(String(10))

    # Embeddings (1536 dimensions for text-embedding-3-small)
    embedding: Mapped[list | None] = mapped_column(Vector(1536))

    # Quality & Status
    quality_score: Mapped[float | None] = mapped_column(Float)
    processing_status: Mapped[ProcessingStatus] = mapped_column(
        Enum(ProcessingStatus), default=ProcessingStatus.PENDING
    )
    error_message: Mapped[str | None] = mapped_column(Text)

    # Metrics
    token_count: Mapped[int | None] = mapped_column(Integer)
    processing_cost_usd: Mapped[float | None] = mapped_column(Float)
```

**Step 4: Update __init__.py**

```python
# backend/app/models/__init__.py
from app.models.base import Base, TimestampMixin
from app.models.document import Document, SourceType, ProcessingStatus

__all__ = ["Base", "TimestampMixin", "Document", "SourceType", "ProcessingStatus"]
```

**Step 5: Run test to verify it passes**

Run: `cd backend && pytest tests/test_models_document.py -v`
Expected: PASS

**Step 6: Commit**

```bash
git add backend/app/models/
git commit -m "feat: add Document model with pgvector embedding"
```

---

### Task 2.2: User Model

**Files:**
- Create: `backend/app/models/user.py`
- Modify: `backend/app/models/__init__.py`
- Test: `backend/tests/test_models_user.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_models_user.py
import pytest
from app.models.user import User, UserRole


def test_user_has_required_fields():
    assert hasattr(User, "id")
    assert hasattr(User, "email")
    assert hasattr(User, "role")
    assert hasattr(User, "api_key")


def test_user_role_enum():
    assert UserRole.USER.value == "user"
    assert UserRole.ADMIN.value == "admin"
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_models_user.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/models/user.py
import enum
from uuid import UUID, uuid4
from sqlalchemy import String, Enum, Boolean
from sqlalchemy.orm import Mapped, mapped_column
from app.models.base import Base, TimestampMixin


class UserRole(str, enum.Enum):
    USER = "user"
    ADMIN = "admin"


class User(Base, TimestampMixin):
    __tablename__ = "users"

    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)
    email: Mapped[str] = mapped_column(String(255), unique=True, nullable=False)
    name: Mapped[str | None] = mapped_column(String(255))
    picture: Mapped[str | None] = mapped_column(String(512))

    role: Mapped[UserRole] = mapped_column(Enum(UserRole), default=UserRole.USER)
    is_active: Mapped[bool] = mapped_column(Boolean, default=True)

    # API key for programmatic access
    api_key: Mapped[str | None] = mapped_column(String(64), unique=True)

    # Google OAuth
    google_id: Mapped[str | None] = mapped_column(String(255), unique=True)
```

**Step 4: Update __init__.py**

```python
# backend/app/models/__init__.py
from app.models.base import Base, TimestampMixin
from app.models.document import Document, SourceType, ProcessingStatus
from app.models.user import User, UserRole

__all__ = [
    "Base", "TimestampMixin",
    "Document", "SourceType", "ProcessingStatus",
    "User", "UserRole",
]
```

**Step 5: Run test to verify it passes**

Run: `cd backend && pytest tests/test_models_user.py -v`
Expected: PASS

**Step 6: Commit**

```bash
git add backend/app/models/
git commit -m "feat: add User model with roles and API key"
```

---

### Task 2.3: Alembic Setup

**Files:**
- Create: `backend/alembic.ini`
- Create: `backend/alembic/env.py`
- Create: `backend/alembic/versions/` (directory)

**Step 1: Initialize Alembic**

Run: `cd backend && alembic init alembic`

**Step 2: Configure alembic.ini**

Edit `backend/alembic.ini`:
```ini
# Change this line:
sqlalchemy.url = driver://user:pass@localhost/dbname
# To:
sqlalchemy.url = postgresql+asyncpg://localhost:5432/hari
```

**Step 3: Configure env.py for async**

Replace `backend/alembic/env.py`:

```python
import asyncio
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

from app.core.config import settings
from app.models import Base

config = context.config
config.set_main_option("sqlalchemy.url", settings.database_url)

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata


def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)
    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)
    await connectable.dispose()


def run_migrations_online() -> None:
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

**Step 4: Create initial migration**

Run: `cd backend && alembic revision --autogenerate -m "initial tables"`

**Step 5: Run migration**

Run: `cd backend && alembic upgrade head`
Expected: Tables `documents` and `users` created

**Step 6: Commit**

```bash
git add backend/alembic/ backend/alembic.ini
git commit -m "feat: add Alembic migrations with async support"
```

---

## Phase 3: Authentication

### Task 3.1: Security Utilities

**Files:**
- Create: `backend/app/core/security.py`
- Test: `backend/tests/test_security.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_security.py
import pytest
from app.core.security import generate_api_key, verify_api_key, create_access_token


def test_generate_api_key_returns_string():
    key = generate_api_key()
    assert isinstance(key, str)
    assert len(key) == 43  # base64 of 32 bytes


def test_generate_api_key_unique():
    key1 = generate_api_key()
    key2 = generate_api_key()
    assert key1 != key2


def test_create_access_token_returns_string():
    token = create_access_token(data={"sub": "test@example.com"})
    assert isinstance(token, str)
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_security.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/core/security.py
import secrets
import base64
from datetime import datetime, timedelta
from jose import jwt
from app.core.config import settings

ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24  # 24 hours


def generate_api_key() -> str:
    return base64.urlsafe_b64encode(secrets.token_bytes(32)).decode().rstrip("=")


def verify_api_key(provided_key: str, stored_key: str) -> bool:
    return secrets.compare_digest(provided_key, stored_key)


def create_access_token(data: dict, expires_delta: timedelta | None = None) -> str:
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, settings.secret_key, algorithm=ALGORITHM)


def decode_access_token(token: str) -> dict | None:
    try:
        return jwt.decode(token, settings.secret_key, algorithms=[ALGORITHM])
    except jwt.JWTError:
        return None
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_security.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/core/security.py backend/tests/test_security.py
git commit -m "feat: add security utilities for API keys and JWT"
```

---

### Task 3.2: Auth Dependencies

**Files:**
- Create: `backend/app/core/deps.py`
- Test: `backend/tests/test_deps.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_deps.py
import pytest
from fastapi import HTTPException
from app.core.deps import get_api_key_header


def test_get_api_key_header_exists():
    assert get_api_key_header is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_deps.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/core/deps.py
from fastapi import Depends, HTTPException, status
from fastapi.security import APIKeyHeader
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.core.database import get_session
from app.core.config import settings
from app.core.security import decode_access_token
from app.models.user import User, UserRole

api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


async def get_api_key_header(
    api_key: str | None = Depends(api_key_header),
) -> str | None:
    return api_key


async def get_current_user(
    session: AsyncSession = Depends(get_session),
    api_key: str | None = Depends(get_api_key_header),
) -> User | None:
    if not api_key:
        return None

    # Check admin API key
    if api_key == settings.admin_api_key:
        # Return a virtual admin user
        return User(email="admin@system", role=UserRole.ADMIN)

    # Check user API key
    result = await session.execute(
        select(User).where(User.api_key == api_key, User.is_active == True)
    )
    return result.scalar_one_or_none()


async def require_user(
    user: User | None = Depends(get_current_user),
) -> User:
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API key",
        )
    return user


async def require_admin(
    user: User = Depends(require_user),
) -> User:
    if user.role != UserRole.ADMIN:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Admin access required",
        )
    return user
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_deps.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/core/deps.py backend/tests/test_deps.py
git commit -m "feat: add auth dependencies for API key and role checks"
```

---

## Phase 4: Document Ingestion Pipeline

### Task 4.1: PDF Extractor

**Files:**
- Create: `backend/app/services/pipeline/__init__.py`
- Create: `backend/app/services/pipeline/pdf_extractor.py`
- Test: `backend/tests/test_pdf_extractor.py`
- Create: `backend/tests/fixtures/sample.pdf` (test fixture)

**Step 1: Write the failing test**

```python
# backend/tests/test_pdf_extractor.py
import pytest
from pathlib import Path
from app.services.pipeline.pdf_extractor import extract_text_from_pdf


@pytest.mark.asyncio
async def test_extract_text_from_pdf_bytes():
    # Create minimal PDF bytes for testing
    pdf_content = b"%PDF-1.4 minimal test"  # Not a real PDF, will test error handling
    result = await extract_text_from_pdf(pdf_content)
    assert isinstance(result, dict)
    assert "text" in result
    assert "error" in result or "page_count" in result
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_pdf_extractor.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/__init__.py
```

```python
# backend/app/services/pipeline/pdf_extractor.py
import io
from PyPDF2 import PdfReader


async def extract_text_from_pdf(pdf_content: bytes) -> dict:
    """Extract text from PDF bytes."""
    try:
        reader = PdfReader(io.BytesIO(pdf_content))
        pages_text = []

        for page in reader.pages:
            text = page.extract_text()
            if text:
                pages_text.append(text.strip())

        full_text = "\n\n".join(pages_text)

        return {
            "text": full_text,
            "page_count": len(reader.pages),
            "metadata": {
                "title": reader.metadata.title if reader.metadata else None,
                "author": reader.metadata.author if reader.metadata else None,
            }
        }
    except Exception as e:
        return {
            "text": "",
            "error": str(e),
        }
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_pdf_extractor.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/
git commit -m "feat: add PDF text extraction"
```

---

### Task 4.2: URL Content Fetcher

**Files:**
- Create: `backend/app/services/pipeline/url_fetcher.py`
- Test: `backend/tests/test_url_fetcher.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_url_fetcher.py
import pytest
from app.services.pipeline.url_fetcher import fetch_url_content


@pytest.mark.asyncio
async def test_fetch_url_content_returns_dict():
    # This will fail in test environment, but tests the interface
    result = await fetch_url_content("https://example.com")
    assert isinstance(result, dict)
    assert "text" in result or "error" in result
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_url_fetcher.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/url_fetcher.py
import httpx
import trafilatura
from trafilatura.settings import use_config

# Configure trafilatura
config = use_config()
config.set("DEFAULT", "EXTRACTION_TIMEOUT", "30")


async def fetch_url_content(url: str) -> dict:
    """Fetch and extract content from URL using Trafilatura."""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
            html = response.text

        # Extract main content
        text = trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
            no_fallback=False,
        )

        # Extract metadata
        metadata = trafilatura.extract_metadata(html)

        return {
            "text": text or "",
            "metadata": {
                "title": metadata.title if metadata else None,
                "author": metadata.author if metadata else None,
                "date": str(metadata.date) if metadata and metadata.date else None,
            },
            "url": str(response.url),  # Final URL after redirects
        }
    except httpx.HTTPError as e:
        return {"text": "", "error": f"HTTP error: {e}"}
    except Exception as e:
        return {"text": "", "error": str(e)}
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_url_fetcher.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/url_fetcher.py
git commit -m "feat: add URL content fetching with Trafilatura"
```

---

### Task 4.3: Text Cleaner

**Files:**
- Create: `backend/app/services/pipeline/text_cleaner.py`
- Test: `backend/tests/test_text_cleaner.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_text_cleaner.py
import pytest
from app.services.pipeline.text_cleaner import clean_text


def test_clean_text_removes_extra_whitespace():
    dirty = "Hello   world\n\n\n\ntest"
    result = clean_text(dirty)
    assert "   " not in result
    assert "\n\n\n\n" not in result


def test_clean_text_preserves_content():
    text = "Hello world. This is a test."
    result = clean_text(text)
    assert "Hello" in result
    assert "test" in result
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_text_cleaner.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/text_cleaner.py
import re


def clean_text(text: str) -> str:
    """Clean and normalize text content."""
    if not text:
        return ""

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)

    # Remove control characters except newlines
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)

    # Normalize line breaks
    text = re.sub(r'\n{3,}', '\n\n', text)

    # Strip leading/trailing whitespace
    text = text.strip()

    return text


def count_tokens(text: str) -> int:
    """Approximate token count (words * 1.3)."""
    if not text:
        return 0
    words = len(text.split())
    return int(words * 1.3)
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_text_cleaner.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/text_cleaner.py
git commit -m "feat: add text cleaning utilities"
```

---

### Task 4.4: Extractive Summarizer (Sumy)

**Files:**
- Create: `backend/app/services/pipeline/extractive_summarizer.py`
- Test: `backend/tests/test_extractive_summarizer.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_extractive_summarizer.py
import pytest
from app.services.pipeline.extractive_summarizer import extractive_summarize


def test_extractive_summarize_returns_string():
    text = "This is the first sentence. This is the second sentence. This is the third sentence. This is the fourth sentence. This is the fifth sentence."
    result = extractive_summarize(text, sentence_count=2)
    assert isinstance(result, str)
    assert len(result) > 0


def test_extractive_summarize_short_text():
    text = "Short text."
    result = extractive_summarize(text, sentence_count=5)
    assert result == text
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_extractive_summarizer.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/extractive_summarizer.py
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words
import nltk

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)


def extractive_summarize(
    text: str,
    sentence_count: int = 10,
    language: str = "english"
) -> str:
    """Extract key sentences using TextRank algorithm."""
    if not text or len(text.split('.')) <= sentence_count:
        return text

    try:
        parser = PlaintextParser.from_string(text, Tokenizer(language))
        stemmer = Stemmer(language)

        # Try TextRank first
        summarizer = TextRankSummarizer(stemmer)
        summarizer.stop_words = get_stop_words(language)

        sentences = summarizer(parser.document, sentence_count)
        summary = " ".join(str(s) for s in sentences)

        return summary if summary else text

    except Exception:
        # Fallback: just take first N sentences
        sentences = text.split('.')[:sentence_count]
        return '.'.join(sentences) + '.' if sentences else text
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_extractive_summarizer.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/extractive_summarizer.py
git commit -m "feat: add extractive summarization with TextRank"
```

---

### Task 4.5: LLM Client Abstraction

**Files:**
- Create: `backend/app/services/llm/__init__.py`
- Create: `backend/app/services/llm/client.py`
- Test: `backend/tests/test_llm_client.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_llm_client.py
import pytest
from app.services.llm.client import LLMClient, LLMProvider


def test_llm_provider_enum():
    assert LLMProvider.ANTHROPIC.value == "anthropic"
    assert LLMProvider.OPENAI.value == "openai"


def test_llm_client_initialization():
    client = LLMClient()
    assert client is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_llm_client.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/llm/__init__.py
from app.services.llm.client import LLMClient, LLMProvider

__all__ = ["LLMClient", "LLMProvider"]
```

```python
# backend/app/services/llm/client.py
import enum
from anthropic import Anthropic
from openai import OpenAI
from app.core.config import settings


class LLMProvider(str, enum.Enum):
    ANTHROPIC = "anthropic"
    OPENAI = "openai"


class LLMClient:
    def __init__(self, provider: LLMProvider = LLMProvider.ANTHROPIC):
        self.provider = provider
        self._anthropic = None
        self._openai = None

    @property
    def anthropic(self) -> Anthropic:
        if not self._anthropic:
            self._anthropic = Anthropic(api_key=settings.anthropic_api_key)
        return self._anthropic

    @property
    def openai(self) -> OpenAI:
        if not self._openai:
            self._openai = OpenAI(api_key=settings.openai_api_key)
        return self._openai

    async def complete(
        self,
        prompt: str,
        system: str = "",
        max_tokens: int = 2000,
        temperature: float = 0.7,
    ) -> dict:
        """Generate completion with automatic fallback."""
        try:
            if self.provider == LLMProvider.ANTHROPIC:
                return await self._complete_anthropic(prompt, system, max_tokens, temperature)
            else:
                return await self._complete_openai(prompt, system, max_tokens, temperature)
        except Exception as e:
            # Try fallback provider
            if self.provider == LLMProvider.ANTHROPIC:
                return await self._complete_openai(prompt, system, max_tokens, temperature)
            raise e

    async def _complete_anthropic(self, prompt: str, system: str, max_tokens: int, temperature: float) -> dict:
        response = self.anthropic.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=max_tokens,
            temperature=temperature,
            system=system,
            messages=[{"role": "user", "content": prompt}]
        )
        return {
            "content": response.content[0].text,
            "provider": "anthropic",
            "model": "claude-sonnet-4-20250514",
            "input_tokens": response.usage.input_tokens,
            "output_tokens": response.usage.output_tokens,
        }

    async def _complete_openai(self, prompt: str, system: str, max_tokens: int, temperature: float) -> dict:
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        response = self.openai.chat.completions.create(
            model="gpt-4-turbo-preview",
            max_tokens=max_tokens,
            temperature=temperature,
            messages=messages
        )
        return {
            "content": response.choices[0].message.content,
            "provider": "openai",
            "model": "gpt-4-turbo-preview",
            "input_tokens": response.usage.prompt_tokens,
            "output_tokens": response.usage.completion_tokens,
        }
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_llm_client.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/llm/
git commit -m "feat: add LLM client with Claude/GPT-4 fallback"
```

---

### Task 4.6: Document Synthesizer

**Files:**
- Create: `backend/app/services/pipeline/synthesizer.py`
- Test: `backend/tests/test_synthesizer.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_synthesizer.py
import pytest
from app.services.pipeline.synthesizer import SYNTHESIS_PROMPT


def test_synthesis_prompt_exists():
    assert SYNTHESIS_PROMPT is not None
    assert "summary" in SYNTHESIS_PROMPT.lower()
    assert "keywords" in SYNTHESIS_PROMPT.lower()
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_synthesizer.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/synthesizer.py
import json
from app.services.llm.client import LLMClient

SYNTHESIS_PROMPT = """Analyze the following text and provide a structured summary.

TEXT:
{text}

Respond with valid JSON only, no other text:
{{
  "summary": "Extended summary (300-500 words) covering main points, key insights, and conclusions",
  "quick_summary": "2-3 sentence executive summary",
  "keywords": ["keyword1", "keyword2", ...],  // 5-10 relevant keywords
  "industries": ["industry1", "industry2"],   // Relevant industry classifications
  "language": "en"  // Detected language code
}}
"""


async def synthesize_document(
    text: str,
    llm_client: LLMClient | None = None
) -> dict:
    """Generate structured summary using LLM."""
    if not text:
        return {"error": "No text provided"}

    client = llm_client or LLMClient()

    prompt = SYNTHESIS_PROMPT.format(text=text[:15000])  # Truncate for token limits

    try:
        response = await client.complete(
            prompt=prompt,
            system="You are a document analysis assistant. Respond only with valid JSON.",
            max_tokens=1500,
            temperature=0.3,
        )

        # Parse JSON response
        content = response["content"]
        # Extract JSON if wrapped in markdown
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]

        result = json.loads(content.strip())
        result["llm_metadata"] = {
            "provider": response["provider"],
            "model": response["model"],
            "input_tokens": response["input_tokens"],
            "output_tokens": response["output_tokens"],
        }
        return result

    except json.JSONDecodeError as e:
        return {"error": f"Failed to parse LLM response: {e}"}
    except Exception as e:
        return {"error": str(e)}
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_synthesizer.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/synthesizer.py
git commit -m "feat: add document synthesis with LLM"
```

---

### Task 4.7: Embeddings Generator

**Files:**
- Create: `backend/app/services/pipeline/embedder.py`
- Test: `backend/tests/test_embedder.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_embedder.py
import pytest
from app.services.pipeline.embedder import EMBEDDING_MODEL, EMBEDDING_DIMENSIONS


def test_embedding_constants():
    assert EMBEDDING_MODEL == "text-embedding-3-small"
    assert EMBEDDING_DIMENSIONS == 1536
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_embedder.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/embedder.py
from openai import OpenAI
from app.core.config import settings

EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536


async def generate_embedding(text: str) -> list[float] | None:
    """Generate embedding vector for text."""
    if not text:
        return None

    try:
        client = OpenAI(api_key=settings.openai_api_key)

        # Truncate text if too long (8191 tokens max)
        truncated = text[:30000]  # Rough character limit

        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=truncated,
        )

        return response.data[0].embedding

    except Exception as e:
        print(f"Embedding error: {e}")
        return None


async def generate_embeddings_batch(texts: list[str]) -> list[list[float] | None]:
    """Generate embeddings for multiple texts."""
    results = []
    for text in texts:
        embedding = await generate_embedding(text)
        results.append(embedding)
    return results
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_embedder.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/embedder.py
git commit -m "feat: add OpenAI embeddings generation"
```

---

### Task 4.8: Quality Scorer

**Files:**
- Create: `backend/app/services/quality/__init__.py`
- Create: `backend/app/services/quality/scorer.py`
- Test: `backend/tests/test_quality_scorer.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_quality_scorer.py
import pytest
from app.services.quality.scorer import calculate_quality_score, QualityGrade


def test_quality_grade_enum():
    assert QualityGrade.A.value == "A"
    assert QualityGrade.D.value == "D"


def test_calculate_quality_score_complete_document():
    score = calculate_quality_score(
        summary="A good summary with enough content" * 20,
        quick_summary="Brief summary",
        keywords=["key1", "key2", "key3", "key4", "key5"],
        industries=["Tech"],
        has_embedding=True,
    )
    assert 0 <= score <= 100
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_quality_scorer.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/quality/__init__.py
from app.services.quality.scorer import calculate_quality_score, QualityGrade

__all__ = ["calculate_quality_score", "QualityGrade"]
```

```python
# backend/app/services/quality/scorer.py
import enum


class QualityGrade(str, enum.Enum):
    A = "A"  # 90+
    B = "B"  # 70-89
    C = "C"  # 50-69
    D = "D"  # <50


def calculate_quality_score(
    summary: str | None = None,
    quick_summary: str | None = None,
    keywords: list | None = None,
    industries: list | None = None,
    has_embedding: bool = False,
) -> float:
    """Calculate quality score (0-100) for a document."""
    score = 0.0

    # Summary quality (40%)
    if summary:
        length = len(summary)
        if 800 <= length <= 2500:
            score += 40
        elif 400 <= length < 800 or 2500 < length <= 4000:
            score += 30
        elif length > 0:
            score += 15

    # Quick summary (10%)
    if quick_summary and len(quick_summary) >= 50:
        score += 10

    # Metadata quality (30%)
    if keywords:
        keyword_count = len(keywords)
        if 5 <= keyword_count <= 10:
            score += 15
        elif keyword_count > 0:
            score += 8

    if industries and len(industries) > 0:
        score += 15

    # Technical quality (20%)
    if has_embedding:
        score += 20

    return min(score, 100.0)


def get_grade(score: float) -> QualityGrade:
    """Convert score to grade."""
    if score >= 90:
        return QualityGrade.A
    elif score >= 70:
        return QualityGrade.B
    elif score >= 50:
        return QualityGrade.C
    else:
        return QualityGrade.D
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_quality_scorer.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/quality/
git commit -m "feat: add document quality scoring"
```

---

### Task 4.9: Pipeline Orchestrator

**Files:**
- Create: `backend/app/services/pipeline/orchestrator.py`
- Test: `backend/tests/test_pipeline_orchestrator.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_pipeline_orchestrator.py
import pytest
from app.services.pipeline.orchestrator import DocumentPipeline


def test_pipeline_initialization():
    pipeline = DocumentPipeline()
    assert pipeline is not None


def test_pipeline_has_process_method():
    pipeline = DocumentPipeline()
    assert hasattr(pipeline, "process_url")
    assert hasattr(pipeline, "process_pdf")
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_pipeline_orchestrator.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/pipeline/orchestrator.py
import hashlib
from app.services.pipeline.url_fetcher import fetch_url_content
from app.services.pipeline.pdf_extractor import extract_text_from_pdf
from app.services.pipeline.text_cleaner import clean_text, count_tokens
from app.services.pipeline.extractive_summarizer import extractive_summarize
from app.services.pipeline.synthesizer import synthesize_document
from app.services.pipeline.embedder import generate_embedding
from app.services.quality.scorer import calculate_quality_score


class DocumentPipeline:
    """Orchestrates the document processing pipeline."""

    async def process_url(self, url: str) -> dict:
        """Process a URL through the full pipeline."""
        # Stage 1: Fetch
        fetch_result = await fetch_url_content(url)
        if "error" in fetch_result:
            return {"status": "failed", "error": fetch_result["error"]}

        return await self._process_text(
            text=fetch_result["text"],
            metadata=fetch_result.get("metadata", {}),
            source_url=url,
        )

    async def process_pdf(self, pdf_content: bytes, filename: str = "") -> dict:
        """Process PDF bytes through the full pipeline."""
        # Stage 1: Extract
        extract_result = await extract_text_from_pdf(pdf_content)
        if "error" in extract_result:
            return {"status": "failed", "error": extract_result["error"]}

        return await self._process_text(
            text=extract_result["text"],
            metadata=extract_result.get("metadata", {}),
            source_url=filename,
        )

    async def _process_text(self, text: str, metadata: dict, source_url: str) -> dict:
        """Process extracted text through remaining pipeline stages."""
        # Stage 2: Clean
        cleaned_text = clean_text(text)
        if not cleaned_text:
            return {"status": "failed", "error": "No content extracted"}

        token_count = count_tokens(cleaned_text)

        # Stage 3: Extractive summary (for long texts)
        if token_count > 2000:
            extractive = extractive_summarize(cleaned_text, sentence_count=30)
        else:
            extractive = cleaned_text

        # Stage 4: LLM synthesis
        synthesis = await synthesize_document(extractive)
        if "error" in synthesis:
            return {"status": "failed", "error": synthesis["error"]}

        # Stage 5: Generate embedding
        embed_text = synthesis.get("summary", cleaned_text[:5000])
        embedding = await generate_embedding(embed_text)

        # Stage 6: Quality scoring
        quality_score = calculate_quality_score(
            summary=synthesis.get("summary"),
            quick_summary=synthesis.get("quick_summary"),
            keywords=synthesis.get("keywords"),
            industries=synthesis.get("industries"),
            has_embedding=embedding is not None,
        )

        # Generate content hash for deduplication
        content_hash = hashlib.sha256(cleaned_text.encode()).hexdigest()

        return {
            "status": "completed",
            "content": cleaned_text,
            "content_hash": content_hash,
            "title": metadata.get("title") or synthesis.get("title"),
            "summary": synthesis.get("summary"),
            "quick_summary": synthesis.get("quick_summary"),
            "keywords": synthesis.get("keywords"),
            "industries": synthesis.get("industries"),
            "language": synthesis.get("language"),
            "embedding": embedding,
            "quality_score": quality_score,
            "token_count": token_count,
            "llm_metadata": synthesis.get("llm_metadata"),
        }
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_pipeline_orchestrator.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/pipeline/orchestrator.py
git commit -m "feat: add document pipeline orchestrator"
```

---

## Phase 4 Complete

The ingestion pipeline is ready. Continue to Phase 5 for Search & Query.

---

## Phase 5: Search & Query

### Task 5.1: Semantic Search Service

**Files:**
- Create: `backend/app/services/search/__init__.py`
- Create: `backend/app/services/search/semantic.py`
- Test: `backend/tests/test_semantic_search.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_semantic_search.py
import pytest
from app.services.search.semantic import SemanticSearch


def test_semantic_search_initialization():
    search = SemanticSearch()
    assert search is not None
    assert hasattr(search, "search")
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_semantic_search.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/search/__init__.py
```

```python
# backend/app/services/search/semantic.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text
from app.models.document import Document, ProcessingStatus
from app.services.pipeline.embedder import generate_embedding


class SemanticSearch:
    def __init__(self, session: AsyncSession | None = None):
        self.session = session

    async def search(
        self,
        query: str,
        limit: int = 10,
        threshold: float = 0.5,
        session: AsyncSession | None = None,
    ) -> list[dict]:
        """Search documents by semantic similarity."""
        db = session or self.session
        if not db:
            raise ValueError("Database session required")

        # Generate query embedding
        query_embedding = await generate_embedding(query)
        if not query_embedding:
            return []

        # pgvector cosine similarity search
        # 1 - cosine_distance gives similarity (0-1)
        sql = text("""
            SELECT
                id,
                title,
                quick_summary,
                keywords,
                url,
                1 - (embedding <=> :query_embedding::vector) as similarity
            FROM documents
            WHERE processing_status = 'completed'
                AND embedding IS NOT NULL
                AND 1 - (embedding <=> :query_embedding::vector) >= :threshold
            ORDER BY embedding <=> :query_embedding::vector
            LIMIT :limit
        """)

        result = await db.execute(
            sql,
            {
                "query_embedding": str(query_embedding),
                "threshold": threshold,
                "limit": limit,
            }
        )

        rows = result.fetchall()
        return [
            {
                "id": str(row.id),
                "title": row.title,
                "quick_summary": row.quick_summary,
                "keywords": row.keywords,
                "url": row.url,
                "similarity": float(row.similarity),
            }
            for row in rows
        ]
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_semantic_search.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/search/
git commit -m "feat: add semantic search with pgvector"
```

---

### Task 5.2: Keyword Search Service

**Files:**
- Create: `backend/app/services/search/keyword.py`
- Test: `backend/tests/test_keyword_search.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_keyword_search.py
import pytest
from app.services.search.keyword import KeywordSearch


def test_keyword_search_initialization():
    search = KeywordSearch()
    assert search is not None
    assert hasattr(search, "search")
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_keyword_search.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/search/keyword.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text


class KeywordSearch:
    def __init__(self, session: AsyncSession | None = None):
        self.session = session

    async def search(
        self,
        query: str,
        limit: int = 10,
        session: AsyncSession | None = None,
    ) -> list[dict]:
        """Search documents using PostgreSQL full-text search."""
        db = session or self.session
        if not db:
            raise ValueError("Database session required")

        # Convert query to tsquery format
        # Simple approach: AND all words together
        words = query.strip().split()
        tsquery = " & ".join(words)

        sql = text("""
            SELECT
                id,
                title,
                quick_summary,
                keywords,
                url,
                ts_rank(
                    to_tsvector('english', coalesce(title, '') || ' ' || coalesce(summary, '')),
                    to_tsquery('english', :tsquery)
                ) as rank
            FROM documents
            WHERE processing_status = 'completed'
                AND to_tsvector('english', coalesce(title, '') || ' ' || coalesce(summary, ''))
                    @@ to_tsquery('english', :tsquery)
            ORDER BY rank DESC
            LIMIT :limit
        """)

        result = await db.execute(sql, {"tsquery": tsquery, "limit": limit})
        rows = result.fetchall()

        return [
            {
                "id": str(row.id),
                "title": row.title,
                "quick_summary": row.quick_summary,
                "keywords": row.keywords,
                "url": row.url,
                "rank": float(row.rank),
            }
            for row in rows
        ]
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_keyword_search.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/search/keyword.py
git commit -m "feat: add keyword search with TSVector"
```

---

### Task 5.3: Hybrid Search with RRF

**Files:**
- Create: `backend/app/services/search/hybrid.py`
- Test: `backend/tests/test_hybrid_search.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_hybrid_search.py
import pytest
from app.services.search.hybrid import HybridSearch, reciprocal_rank_fusion


def test_rrf_empty_lists():
    result = reciprocal_rank_fusion([], [])
    assert result == []


def test_rrf_single_list():
    items = [{"id": "1"}, {"id": "2"}]
    result = reciprocal_rank_fusion(items, [])
    assert len(result) == 2


def test_hybrid_search_initialization():
    search = HybridSearch()
    assert search is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_hybrid_search.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/search/hybrid.py
from sqlalchemy.ext.asyncio import AsyncSession
from app.services.search.semantic import SemanticSearch
from app.services.search.keyword import KeywordSearch


def reciprocal_rank_fusion(
    *result_lists: list[dict],
    k: int = 60,
) -> list[dict]:
    """Combine multiple ranked lists using RRF."""
    scores: dict[str, float] = {}
    items: dict[str, dict] = {}

    for result_list in result_lists:
        for rank, item in enumerate(result_list):
            doc_id = item["id"]
            rrf_score = 1.0 / (k + rank + 1)
            scores[doc_id] = scores.get(doc_id, 0) + rrf_score
            if doc_id not in items:
                items[doc_id] = item

    # Sort by combined RRF score
    sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

    return [
        {**items[doc_id], "rrf_score": scores[doc_id]}
        for doc_id in sorted_ids
    ]


class HybridSearch:
    def __init__(self, session: AsyncSession | None = None):
        self.session = session
        self.semantic = SemanticSearch(session)
        self.keyword = KeywordSearch(session)

    async def search(
        self,
        query: str,
        limit: int = 10,
        semantic_weight: float = 0.7,
        session: AsyncSession | None = None,
    ) -> list[dict]:
        """Hybrid search combining semantic and keyword search."""
        db = session or self.session

        # Run both searches
        semantic_results = await self.semantic.search(
            query, limit=limit * 2, session=db
        )
        keyword_results = await self.keyword.search(
            query, limit=limit * 2, session=db
        )

        # Combine with RRF
        combined = reciprocal_rank_fusion(semantic_results, keyword_results)

        return combined[:limit]
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_hybrid_search.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/search/hybrid.py
git commit -m "feat: add hybrid search with RRF fusion"
```

---

### Task 5.4: Query Generator

**Files:**
- Create: `backend/app/services/query/__init__.py`
- Create: `backend/app/services/query/generator.py`
- Test: `backend/tests/test_query_generator.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_query_generator.py
import pytest
from app.services.query.generator import RESPONSE_PROMPT


def test_response_prompt_exists():
    assert RESPONSE_PROMPT is not None
    assert "context" in RESPONSE_PROMPT.lower()
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_query_generator.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/services/query/__init__.py
```

```python
# backend/app/services/query/generator.py
from app.services.llm.client import LLMClient

RESPONSE_PROMPT = """You are HARI, a knowledge assistant. Answer the user's question based on the provided context.

CONTEXT:
{context}

USER QUESTION:
{question}

Instructions:
- Answer based primarily on the provided context
- If the context doesn't contain enough information, acknowledge this
- Be concise and direct
- Cite sources when relevant by mentioning document titles

RESPONSE:
"""


async def generate_response(
    question: str,
    context: list[dict],
    llm_client: LLMClient | None = None,
) -> dict:
    """Generate a response using retrieved context."""
    client = llm_client or LLMClient()

    # Format context
    context_text = "\n\n".join([
        f"[{doc.get('title', 'Untitled')}]\n{doc.get('quick_summary', doc.get('summary', ''))}"
        for doc in context
    ])

    if not context_text:
        context_text = "No relevant documents found."

    prompt = RESPONSE_PROMPT.format(
        context=context_text,
        question=question,
    )

    try:
        response = await client.complete(
            prompt=prompt,
            system="You are HARI, a helpful knowledge assistant.",
            max_tokens=1000,
            temperature=0.7,
        )

        return {
            "answer": response["content"],
            "sources": [
                {"id": doc.get("id"), "title": doc.get("title"), "url": doc.get("url")}
                for doc in context
            ],
            "llm_metadata": {
                "provider": response["provider"],
                "model": response["model"],
            }
        }
    except Exception as e:
        return {"error": str(e)}
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_query_generator.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/query/
git commit -m "feat: add RAG response generation"
```

---

## Phase 5 Complete

Search and query generation ready. Continue to Phase 6 for FastAPI endpoints.

---

## Phase 6: FastAPI Application

### Task 6.1: Pydantic Schemas

**Files:**
- Create: `backend/app/schemas/__init__.py`
- Create: `backend/app/schemas/document.py`
- Create: `backend/app/schemas/query.py`
- Test: `backend/tests/test_schemas.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_schemas.py
import pytest
from app.schemas.document import DocumentCreate, DocumentResponse
from app.schemas.query import QueryRequest, QueryResponse


def test_document_create_schema():
    doc = DocumentCreate(url="https://example.com")
    assert doc.url == "https://example.com"


def test_query_request_schema():
    query = QueryRequest(query="test question")
    assert query.query == "test question"
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_schemas.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/schemas/__init__.py
```

```python
# backend/app/schemas/document.py
from pydantic import BaseModel, HttpUrl
from uuid import UUID
from datetime import datetime


class DocumentCreate(BaseModel):
    url: str | None = None


class DocumentResponse(BaseModel):
    id: UUID
    url: str | None
    title: str | None
    quick_summary: str | None
    keywords: list[str] | None
    industries: list[str] | None
    quality_score: float | None
    processing_status: str
    created_at: datetime

    class Config:
        from_attributes = True


class DocumentDetail(DocumentResponse):
    summary: str | None
    content: str | None
    language: str | None
    error_message: str | None


class DocumentList(BaseModel):
    items: list[DocumentResponse]
    total: int
    page: int
    page_size: int
```

```python
# backend/app/schemas/query.py
from pydantic import BaseModel


class QueryRequest(BaseModel):
    query: str
    limit: int = 5


class SourceReference(BaseModel):
    id: str | None
    title: str | None
    url: str | None


class QueryResponse(BaseModel):
    answer: str
    sources: list[SourceReference]


class SearchRequest(BaseModel):
    query: str
    limit: int = 10
    threshold: float = 0.5


class SearchResult(BaseModel):
    id: str
    title: str | None
    quick_summary: str | None
    url: str | None
    score: float
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_schemas.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/schemas/
git commit -m "feat: add Pydantic schemas for API"
```

---

### Task 6.2: Document API Routes

**Files:**
- Create: `backend/app/api/__init__.py`
- Create: `backend/app/api/documents.py`
- Test: `backend/tests/test_api_documents.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_api_documents.py
import pytest
from fastapi.testclient import TestClient
from app.main import app


def test_documents_router_exists():
    from app.api.documents import router
    assert router is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_api_documents.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/api/__init__.py
```

```python
# backend/app/api/documents.py
from uuid import UUID
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func

from app.core.database import get_session
from app.core.deps import require_user
from app.models.document import Document, SourceType, ProcessingStatus
from app.models.user import User
from app.schemas.document import DocumentCreate, DocumentResponse, DocumentDetail, DocumentList
from app.services.pipeline.orchestrator import DocumentPipeline

router = APIRouter(prefix="/documents", tags=["documents"])


@router.post("/", response_model=DocumentResponse, status_code=status.HTTP_201_CREATED)
async def create_document_from_url(
    data: DocumentCreate,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Process a URL and create a document."""
    if not data.url:
        raise HTTPException(status_code=400, detail="URL required")

    # Check for duplicate
    existing = await session.execute(
        select(Document).where(Document.url == data.url)
    )
    if existing.scalar_one_or_none():
        raise HTTPException(status_code=409, detail="Document already exists")

    # Create document record
    doc = Document(url=data.url, source_type=SourceType.URL, processing_status=ProcessingStatus.PROCESSING)
    session.add(doc)
    await session.commit()

    # Process through pipeline
    pipeline = DocumentPipeline()
    result = await pipeline.process_url(data.url)

    # Update document with results
    if result["status"] == "completed":
        doc.processing_status = ProcessingStatus.COMPLETED
        doc.content = result.get("content")
        doc.content_hash = result.get("content_hash")
        doc.title = result.get("title")
        doc.summary = result.get("summary")
        doc.quick_summary = result.get("quick_summary")
        doc.keywords = result.get("keywords")
        doc.industries = result.get("industries")
        doc.language = result.get("language")
        doc.embedding = result.get("embedding")
        doc.quality_score = result.get("quality_score")
        doc.token_count = result.get("token_count")
    else:
        doc.processing_status = ProcessingStatus.FAILED
        doc.error_message = result.get("error")

    await session.commit()
    await session.refresh(doc)

    return doc


@router.post("/upload", response_model=DocumentResponse, status_code=status.HTTP_201_CREATED)
async def upload_pdf(
    file: UploadFile = File(...),
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Upload and process a PDF file."""
    if not file.filename or not file.filename.lower().endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files accepted")

    content = await file.read()

    # Create document record
    doc = Document(
        url=file.filename,
        source_type=SourceType.PDF,
        processing_status=ProcessingStatus.PROCESSING
    )
    session.add(doc)
    await session.commit()

    # Process through pipeline
    pipeline = DocumentPipeline()
    result = await pipeline.process_pdf(content, file.filename)

    # Update document with results
    if result["status"] == "completed":
        doc.processing_status = ProcessingStatus.COMPLETED
        doc.content = result.get("content")
        doc.content_hash = result.get("content_hash")
        doc.title = result.get("title") or file.filename
        doc.summary = result.get("summary")
        doc.quick_summary = result.get("quick_summary")
        doc.keywords = result.get("keywords")
        doc.industries = result.get("industries")
        doc.language = result.get("language")
        doc.embedding = result.get("embedding")
        doc.quality_score = result.get("quality_score")
        doc.token_count = result.get("token_count")
    else:
        doc.processing_status = ProcessingStatus.FAILED
        doc.error_message = result.get("error")

    await session.commit()
    await session.refresh(doc)

    return doc


@router.get("/", response_model=DocumentList)
async def list_documents(
    page: int = 1,
    page_size: int = 20,
    status: str | None = None,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """List all documents with pagination."""
    query = select(Document).order_by(Document.created_at.desc())

    if status:
        query = query.where(Document.processing_status == status)

    # Count total
    count_query = select(func.count()).select_from(Document)
    if status:
        count_query = count_query.where(Document.processing_status == status)
    total = (await session.execute(count_query)).scalar() or 0

    # Paginate
    query = query.offset((page - 1) * page_size).limit(page_size)
    result = await session.execute(query)
    docs = result.scalars().all()

    return DocumentList(items=docs, total=total, page=page, page_size=page_size)


@router.get("/{document_id}", response_model=DocumentDetail)
async def get_document(
    document_id: UUID,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Get a single document by ID."""
    result = await session.execute(select(Document).where(Document.id == document_id))
    doc = result.scalar_one_or_none()

    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    return doc


@router.delete("/{document_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_document(
    document_id: UUID,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Delete a document."""
    result = await session.execute(select(Document).where(Document.id == document_id))
    doc = result.scalar_one_or_none()

    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    await session.delete(doc)
    await session.commit()
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_api_documents.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/api/documents.py
git commit -m "feat: add document API endpoints"
```

---

### Task 6.3: Search and Query API Routes

**Files:**
- Create: `backend/app/api/search.py`
- Create: `backend/app/api/query.py`
- Test: `backend/tests/test_api_search.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_api_search.py
import pytest


def test_search_router_exists():
    from app.api.search import router
    assert router is not None


def test_query_router_exists():
    from app.api.query import router
    assert router is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_api_search.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/api/search.py
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_session
from app.core.deps import require_user
from app.models.user import User
from app.schemas.query import SearchRequest, SearchResult
from app.services.search.hybrid import HybridSearch

router = APIRouter(prefix="/search", tags=["search"])


@router.post("/", response_model=list[SearchResult])
async def search_documents(
    data: SearchRequest,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Search documents using hybrid search."""
    search = HybridSearch(session)
    results = await search.search(
        query=data.query,
        limit=data.limit,
        session=session,
    )

    return [
        SearchResult(
            id=r["id"],
            title=r.get("title"),
            quick_summary=r.get("quick_summary"),
            url=r.get("url"),
            score=r.get("rrf_score", r.get("similarity", 0)),
        )
        for r in results
    ]
```

```python
# backend/app/api/query.py
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_session
from app.core.deps import require_user
from app.models.user import User
from app.schemas.query import QueryRequest, QueryResponse, SourceReference
from app.services.search.hybrid import HybridSearch
from app.services.query.generator import generate_response

router = APIRouter(prefix="/query", tags=["query"])


@router.post("/", response_model=QueryResponse)
async def query_knowledge_base(
    data: QueryRequest,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_user),
):
    """Query the knowledge base with RAG."""
    # Search for relevant documents
    search = HybridSearch(session)
    results = await search.search(
        query=data.query,
        limit=data.limit,
        session=session,
    )

    # Generate response
    response = await generate_response(
        question=data.query,
        context=results,
    )

    if "error" in response:
        return QueryResponse(answer=f"Error: {response['error']}", sources=[])

    return QueryResponse(
        answer=response["answer"],
        sources=[
            SourceReference(id=s.get("id"), title=s.get("title"), url=s.get("url"))
            for s in response.get("sources", [])
        ],
    )
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_api_search.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/api/search.py backend/app/api/query.py
git commit -m "feat: add search and query API endpoints"
```

---

### Task 6.4: Admin API Routes

**Files:**
- Create: `backend/app/api/admin.py`
- Test: `backend/tests/test_api_admin.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_api_admin.py
import pytest


def test_admin_router_exists():
    from app.api.admin import router
    assert router is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_api_admin.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/api/admin.py
from uuid import UUID
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func

from app.core.database import get_session
from app.core.deps import require_admin
from app.models.document import Document, ProcessingStatus
from app.models.user import User
from app.services.quality.scorer import get_grade, QualityGrade

router = APIRouter(prefix="/admin", tags=["admin"])


@router.get("/quality/report")
async def quality_report(
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_admin),
):
    """Get quality distribution report."""
    result = await session.execute(
        select(Document).where(Document.processing_status == ProcessingStatus.COMPLETED)
    )
    docs = result.scalars().all()

    grades = {"A": 0, "B": 0, "C": 0, "D": 0}
    for doc in docs:
        if doc.quality_score is not None:
            grade = get_grade(doc.quality_score)
            grades[grade.value] += 1

    total = len(docs)
    return {
        "total_documents": total,
        "grade_distribution": grades,
        "average_score": sum(d.quality_score or 0 for d in docs) / total if total > 0 else 0,
    }


@router.get("/documents/failed")
async def list_failed_documents(
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_admin),
):
    """List all failed documents."""
    result = await session.execute(
        select(Document)
        .where(Document.processing_status == ProcessingStatus.FAILED)
        .order_by(Document.created_at.desc())
    )
    docs = result.scalars().all()

    return [
        {
            "id": str(doc.id),
            "url": doc.url,
            "error_message": doc.error_message,
            "created_at": doc.created_at,
        }
        for doc in docs
    ]


@router.post("/documents/{document_id}/retry")
async def retry_document(
    document_id: UUID,
    session: AsyncSession = Depends(get_session),
    user: User = Depends(require_admin),
):
    """Retry processing a failed document."""
    result = await session.execute(select(Document).where(Document.id == document_id))
    doc = result.scalar_one_or_none()

    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    if doc.processing_status != ProcessingStatus.FAILED:
        raise HTTPException(status_code=400, detail="Document is not in failed state")

    # Reset and reprocess
    from app.services.pipeline.orchestrator import DocumentPipeline

    doc.processing_status = ProcessingStatus.PROCESSING
    doc.error_message = None
    await session.commit()

    pipeline = DocumentPipeline()

    if doc.source_type.value == "url":
        result = await pipeline.process_url(doc.url)
    else:
        raise HTTPException(status_code=400, detail="Cannot retry non-URL documents without original file")

    if result["status"] == "completed":
        doc.processing_status = ProcessingStatus.COMPLETED
        doc.content = result.get("content")
        doc.summary = result.get("summary")
        doc.quick_summary = result.get("quick_summary")
        doc.keywords = result.get("keywords")
        doc.industries = result.get("industries")
        doc.embedding = result.get("embedding")
        doc.quality_score = result.get("quality_score")
    else:
        doc.processing_status = ProcessingStatus.FAILED
        doc.error_message = result.get("error")

    await session.commit()
    await session.refresh(doc)

    return {"status": doc.processing_status.value, "quality_score": doc.quality_score}
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_api_admin.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/api/admin.py
git commit -m "feat: add admin API endpoints"
```

---

### Task 6.5: FastAPI Main Application

**Files:**
- Create: `backend/app/main.py`
- Test: `backend/tests/test_main.py`

**Step 1: Write the failing test**

```python
# backend/tests/test_main.py
import pytest
from fastapi.testclient import TestClient
from app.main import app


client = TestClient(app)


def test_health_endpoint():
    response = client.get("/health")
    assert response.status_code == 200
    assert "status" in response.json()
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_main.py -v`
Expected: FAIL

**Step 3: Write minimal implementation**

```python
# backend/app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.core.config import settings
from app.api.documents import router as documents_router
from app.api.search import router as search_router
from app.api.query import router as query_router
from app.api.admin import router as admin_router

app = FastAPI(
    title=settings.app_name,
    description="Human-Augmented Resource Intelligence API",
    version="0.1.0",
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(documents_router, prefix="/api")
app.include_router(search_router, prefix="/api")
app.include_router(query_router, prefix="/api")
app.include_router(admin_router, prefix="/api")


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "0.1.0",
        "environment": settings.environment,
    }
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_main.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/main.py
git commit -m "feat: add FastAPI main application"
```

---

## Phase 6 Complete

Backend API is ready. Continue to Phase 7 for Frontend Setup.

---

## Phase 7: Frontend Setup

### Task 7.1: Initialize React Project

**Files:**
- Create: `frontend/package.json`
- Create: `frontend/vite.config.ts`
- Create: `frontend/tsconfig.json`

**Step 1: Create frontend with Vite**

```bash
cd /path/to/hari2
npm create vite@latest frontend -- --template react-ts
cd frontend
npm install
```

**Step 2: Install dependencies**

```bash
npm install @tanstack/react-query axios react-router-dom
npm install -D tailwindcss postcss autoprefixer @types/node
npx tailwindcss init -p
```

**Step 3: Configure Tailwind**

Update `frontend/tailwind.config.js`:
```javascript
/** @type {import('tailwindcss').Config} */
export default {
  darkMode: ["class"],
  content: ["./index.html", "./src/**/*.{ts,tsx,js,jsx}"],
  theme: {
    extend: {},
  },
  plugins: [],
}
```

Update `frontend/src/index.css`:
```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

**Step 4: Commit**

```bash
git add frontend/
git commit -m "feat: initialize React frontend with Vite and Tailwind"
```

---

### Task 7.2: Install shadcn/ui

**Step 1: Initialize shadcn/ui**

```bash
cd frontend
npx shadcn@latest init
```

Choose options:
- Style: Default
- Base color: Slate
- CSS variables: Yes

**Step 2: Add core components**

```bash
npx shadcn@latest add button card input textarea
npx shadcn@latest add table badge separator scroll-area
```

**Step 3: Commit**

```bash
git add frontend/
git commit -m "feat: add shadcn/ui components"
```

---

### Task 7.3: API Client Setup

**Files:**
- Create: `frontend/src/lib/api.ts`

**Step 1: Create API client**

```typescript
// frontend/src/lib/api.ts
import axios from 'axios';

const API_BASE = import.meta.env.VITE_API_URL || 'http://localhost:8000';

export const api = axios.create({
  baseURL: API_BASE,
  headers: {
    'Content-Type': 'application/json',
  },
});

// Add API key to requests
api.interceptors.request.use((config) => {
  const apiKey = localStorage.getItem('api_key');
  if (apiKey) {
    config.headers['X-API-Key'] = apiKey;
  }
  return config;
});

// API functions
export const queryApi = {
  ask: (query: string, limit = 5) =>
    api.post('/api/query/', { query, limit }),

  search: (query: string, limit = 10) =>
    api.post('/api/search/', { query, limit }),
};

export const documentsApi = {
  list: (page = 1, pageSize = 20, status?: string) =>
    api.get('/api/documents/', { params: { page, page_size: pageSize, status } }),

  get: (id: string) =>
    api.get(`/api/documents/${id}`),

  create: (url: string) =>
    api.post('/api/documents/', { url }),

  uploadPdf: (file: File) => {
    const formData = new FormData();
    formData.append('file', file);
    return api.post('/api/documents/upload', formData, {
      headers: { 'Content-Type': 'multipart/form-data' },
    });
  },

  delete: (id: string) =>
    api.delete(`/api/documents/${id}`),
};

export const adminApi = {
  qualityReport: () =>
    api.get('/api/admin/quality/report'),

  failedDocuments: () =>
    api.get('/api/admin/documents/failed'),

  retryDocument: (id: string) =>
    api.post(`/api/admin/documents/${id}/retry`),
};
```

**Step 2: Commit**

```bash
git add frontend/src/lib/api.ts
git commit -m "feat: add API client"
```

---

### Task 7.4: React Query Setup

**Files:**
- Create: `frontend/src/lib/query-client.ts`
- Modify: `frontend/src/main.tsx`

**Step 1: Create query client**

```typescript
// frontend/src/lib/query-client.ts
import { QueryClient } from '@tanstack/react-query';

export const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 1000 * 60, // 1 minute
      retry: 1,
    },
  },
});
```

**Step 2: Update main.tsx**

```typescript
// frontend/src/main.tsx
import React from 'react';
import ReactDOM from 'react-dom/client';
import { QueryClientProvider } from '@tanstack/react-query';
import { BrowserRouter } from 'react-router-dom';
import { queryClient } from './lib/query-client';
import App from './App';
import './index.css';

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <QueryClientProvider client={queryClient}>
      <BrowserRouter>
        <App />
      </BrowserRouter>
    </QueryClientProvider>
  </React.StrictMode>,
);
```

**Step 3: Commit**

```bash
git add frontend/src/lib/query-client.ts frontend/src/main.tsx
git commit -m "feat: add React Query setup"
```

---

## Phase 8: Chat & Admin UI

### Task 8.1: Layout Components

**Files:**
- Create: `frontend/src/components/layout/Header.tsx`
- Create: `frontend/src/components/layout/Layout.tsx`

**Step 1: Create Header**

```typescript
// frontend/src/components/layout/Header.tsx
import { Link, useLocation } from 'react-router-dom';
import { cn } from '@/lib/utils';

export function Header() {
  const location = useLocation();

  const links = [
    { href: '/', label: 'Chat' },
    { href: '/admin', label: 'Admin' },
  ];

  return (
    <header className="border-b">
      <div className="container mx-auto px-4 h-14 flex items-center justify-between">
        <Link to="/" className="font-bold text-xl">HARI</Link>
        <nav className="flex gap-4">
          {links.map(link => (
            <Link
              key={link.href}
              to={link.href}
              className={cn(
                "text-sm font-medium transition-colors hover:text-primary",
                location.pathname === link.href ? "text-primary" : "text-muted-foreground"
              )}
            >
              {link.label}
            </Link>
          ))}
        </nav>
      </div>
    </header>
  );
}
```

**Step 2: Create Layout**

```typescript
// frontend/src/components/layout/Layout.tsx
import { Header } from './Header';

export function Layout({ children }: { children: React.ReactNode }) {
  return (
    <div className="min-h-screen flex flex-col">
      <Header />
      <main className="flex-1 container mx-auto px-4 py-6">
        {children}
      </main>
    </div>
  );
}
```

**Step 3: Commit**

```bash
git add frontend/src/components/layout/
git commit -m "feat: add layout components"
```

---

### Task 8.2: Chat Interface

**Files:**
- Create: `frontend/src/pages/ChatPage.tsx`
- Create: `frontend/src/components/chat/ChatInput.tsx`
- Create: `frontend/src/components/chat/ChatMessage.tsx`

**Step 1: Create ChatInput**

```typescript
// frontend/src/components/chat/ChatInput.tsx
import { useState } from 'react';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';

interface ChatInputProps {
  onSubmit: (message: string) => void;
  isLoading: boolean;
}

export function ChatInput({ onSubmit, isLoading }: ChatInputProps) {
  const [message, setMessage] = useState('');

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (message.trim() && !isLoading) {
      onSubmit(message);
      setMessage('');
    }
  };

  return (
    <form onSubmit={handleSubmit} className="flex gap-2">
      <Textarea
        value={message}
        onChange={(e) => setMessage(e.target.value)}
        placeholder="Ask a question..."
        className="flex-1 min-h-[60px] resize-none"
        onKeyDown={(e) => {
          if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSubmit(e);
          }
        }}
      />
      <Button type="submit" disabled={isLoading || !message.trim()}>
        {isLoading ? 'Thinking...' : 'Ask'}
      </Button>
    </form>
  );
}
```

**Step 2: Create ChatMessage**

```typescript
// frontend/src/components/chat/ChatMessage.tsx
import { Card } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';

interface Source {
  id: string | null;
  title: string | null;
  url: string | null;
}

interface ChatMessageProps {
  role: 'user' | 'assistant';
  content: string;
  sources?: Source[];
}

export function ChatMessage({ role, content, sources }: ChatMessageProps) {
  return (
    <div className={`flex ${role === 'user' ? 'justify-end' : 'justify-start'}`}>
      <Card className={`max-w-[80%] p-4 ${role === 'user' ? 'bg-primary text-primary-foreground' : ''}`}>
        <p className="whitespace-pre-wrap">{content}</p>
        {sources && sources.length > 0 && (
          <div className="mt-3 pt-3 border-t">
            <p className="text-xs text-muted-foreground mb-2">Sources:</p>
            <div className="flex flex-wrap gap-1">
              {sources.map((source, i) => (
                <Badge key={i} variant="outline" className="text-xs">
                  {source.title || 'Untitled'}
                </Badge>
              ))}
            </div>
          </div>
        )}
      </Card>
    </div>
  );
}
```

**Step 3: Create ChatPage**

```typescript
// frontend/src/pages/ChatPage.tsx
import { useState } from 'react';
import { useMutation } from '@tanstack/react-query';
import { queryApi } from '@/lib/api';
import { ChatInput } from '@/components/chat/ChatInput';
import { ChatMessage } from '@/components/chat/ChatMessage';
import { ScrollArea } from '@/components/ui/scroll-area';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  sources?: { id: string | null; title: string | null; url: string | null }[];
}

export function ChatPage() {
  const [messages, setMessages] = useState<Message[]>([]);

  const queryMutation = useMutation({
    mutationFn: (query: string) => queryApi.ask(query),
    onSuccess: (response) => {
      setMessages(prev => [
        ...prev,
        {
          role: 'assistant',
          content: response.data.answer,
          sources: response.data.sources,
        },
      ]);
    },
    onError: (error: Error) => {
      setMessages(prev => [
        ...prev,
        { role: 'assistant', content: `Error: ${error.message}` },
      ]);
    },
  });

  const handleSubmit = (message: string) => {
    setMessages(prev => [...prev, { role: 'user', content: message }]);
    queryMutation.mutate(message);
  };

  return (
    <div className="flex flex-col h-[calc(100vh-8rem)]">
      <ScrollArea className="flex-1 pr-4">
        <div className="space-y-4 pb-4">
          {messages.length === 0 ? (
            <div className="text-center text-muted-foreground py-12">
              <h2 className="text-2xl font-semibold mb-2">Welcome to HARI</h2>
              <p>Ask any question about your knowledge base</p>
            </div>
          ) : (
            messages.map((msg, i) => (
              <ChatMessage key={i} {...msg} />
            ))
          )}
        </div>
      </ScrollArea>
      <div className="pt-4 border-t">
        <ChatInput onSubmit={handleSubmit} isLoading={queryMutation.isPending} />
      </div>
    </div>
  );
}
```

**Step 4: Commit**

```bash
git add frontend/src/pages/ChatPage.tsx frontend/src/components/chat/
git commit -m "feat: add chat interface"
```

---

### Task 8.3: Admin Dashboard

**Files:**
- Create: `frontend/src/pages/AdminPage.tsx`
- Create: `frontend/src/components/admin/DocumentsTable.tsx`
- Create: `frontend/src/components/admin/QualityChart.tsx`

**Step 1: Create DocumentsTable**

```typescript
// frontend/src/components/admin/DocumentsTable.tsx
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { documentsApi } from '@/lib/api';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '@/components/ui/table';
import { Badge } from '@/components/ui/badge';
import { Button } from '@/components/ui/button';

export function DocumentsTable() {
  const queryClient = useQueryClient();

  const { data, isLoading } = useQuery({
    queryKey: ['documents'],
    queryFn: () => documentsApi.list(),
  });

  const deleteMutation = useMutation({
    mutationFn: (id: string) => documentsApi.delete(id),
    onSuccess: () => queryClient.invalidateQueries({ queryKey: ['documents'] }),
  });

  if (isLoading) return <div>Loading...</div>;

  const documents = data?.data.items || [];

  return (
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Title</TableHead>
          <TableHead>Status</TableHead>
          <TableHead>Quality</TableHead>
          <TableHead>Actions</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        {documents.map((doc: any) => (
          <TableRow key={doc.id}>
            <TableCell className="font-medium">{doc.title || doc.url || 'Untitled'}</TableCell>
            <TableCell>
              <Badge variant={doc.processing_status === 'completed' ? 'default' : 'destructive'}>
                {doc.processing_status}
              </Badge>
            </TableCell>
            <TableCell>{doc.quality_score?.toFixed(0) || '-'}</TableCell>
            <TableCell>
              <Button
                variant="destructive"
                size="sm"
                onClick={() => deleteMutation.mutate(doc.id)}
              >
                Delete
              </Button>
            </TableCell>
          </TableRow>
        ))}
      </TableBody>
    </Table>
  );
}
```

**Step 2: Create QualityChart**

```typescript
// frontend/src/components/admin/QualityChart.tsx
import { useQuery } from '@tanstack/react-query';
import { adminApi } from '@/lib/api';
import { Card } from '@/components/ui/card';

export function QualityChart() {
  const { data, isLoading } = useQuery({
    queryKey: ['quality-report'],
    queryFn: () => adminApi.qualityReport(),
  });

  if (isLoading) return <div>Loading...</div>;

  const report = data?.data;
  const grades = report?.grade_distribution || {};

  return (
    <Card className="p-4">
      <h3 className="font-semibold mb-4">Quality Distribution</h3>
      <div className="flex gap-4">
        {['A', 'B', 'C', 'D'].map(grade => (
          <div key={grade} className="text-center">
            <div className="text-2xl font-bold">{grades[grade] || 0}</div>
            <div className="text-sm text-muted-foreground">Grade {grade}</div>
          </div>
        ))}
      </div>
      <div className="mt-4 pt-4 border-t">
        <p className="text-sm text-muted-foreground">
          Total: {report?.total_documents || 0} documents |
          Avg Score: {report?.average_score?.toFixed(1) || 0}
        </p>
      </div>
    </Card>
  );
}
```

**Step 3: Create AdminPage**

```typescript
// frontend/src/pages/AdminPage.tsx
import { DocumentsTable } from '@/components/admin/DocumentsTable';
import { QualityChart } from '@/components/admin/QualityChart';
import { Separator } from '@/components/ui/separator';

export function AdminPage() {
  return (
    <div className="space-y-6">
      <div>
        <h1 className="text-2xl font-bold">Admin Dashboard</h1>
        <p className="text-muted-foreground">Manage documents and monitor quality</p>
      </div>

      <QualityChart />

      <Separator />

      <div>
        <h2 className="text-xl font-semibold mb-4">Documents</h2>
        <DocumentsTable />
      </div>
    </div>
  );
}
```

**Step 4: Commit**

```bash
git add frontend/src/pages/AdminPage.tsx frontend/src/components/admin/
git commit -m "feat: add admin dashboard"
```

---

### Task 8.4: App Routing

**Files:**
- Modify: `frontend/src/App.tsx`

**Step 1: Update App.tsx**

```typescript
// frontend/src/App.tsx
import { Routes, Route } from 'react-router-dom';
import { Layout } from '@/components/layout/Layout';
import { ChatPage } from '@/pages/ChatPage';
import { AdminPage } from '@/pages/AdminPage';

function App() {
  return (
    <Layout>
      <Routes>
        <Route path="/" element={<ChatPage />} />
        <Route path="/admin" element={<AdminPage />} />
      </Routes>
    </Layout>
  );
}

export default App;
```

**Step 2: Commit**

```bash
git add frontend/src/App.tsx
git commit -m "feat: add app routing"
```

---

## Plan Complete

All 8 phases cover the HARI MVP:

| Phase | Tasks | Focus |
|-------|-------|-------|
| 1 | 1.1-1.4 | Backend project setup |
| 2 | 2.1-2.3 | Database models + migrations |
| 3 | 3.1-3.2 | Authentication |
| 4 | 4.1-4.9 | Document ingestion pipeline |
| 5 | 5.1-5.4 | Search & query |
| 6 | 6.1-6.5 | FastAPI application |
| 7 | 7.1-7.4 | Frontend setup |
| 8 | 8.1-8.4 | Chat & admin UI |

**Total: ~35 tasks**

---

## Execution Handoff

Plan complete and saved to `docs/plans/2025-01-22-hari-mvp.md`.

**Two execution options:**

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

**Which approach?**
